{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Test](https://d2l.ai/_images/softmaxreg.svg)  \n",
    "This can be represtented by function `o = Wx + b` where `W` is a **3x4** matrix represetnting all of the weights (or arrows in this case) and b is an element of R^3 (R being the set of all real numbers)  \n",
    "This means that `b` is a **1x3** vector and `x` is also a **1x3** vector. This means that `o` is a **1x3** vector as well. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "y-hat = `softmax(o)` where `y-hat_i = e^(o_i)/SIGMA_j(e^(o_j))`   \n",
    "The softmax is [monotonic](https://en.wikipedia.org/wiki/Monotonic_function) and non negative as the limit as x -> -infinity of e^x is 0. The softmax is also normalized using every output to get the probability. In this case, the largest o_i is the most likely class according to y-hat\n",
    "### Argmax\n",
    "argmax is the \"hard classification\" alternative to softmax where we take the highest value of all of the outputs, using instead a **max** function of each output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "To improve efficiency we take the number of examples that we have and apply that to the input vector x such that X is an element of `R^nxd` where n is the number of examples and d is the number of inputs. Our function is now `O = XW + b` where W is an element of `R^dxq` and b is an element of `R^1xq`  \n",
    "The result of XW is an element of `R^nxq` (where q is the output categories) and vector b is added to each row of XW leaving a nxq matrix for O.  \n",
    "The output goes through the same softmax but it is handled *rowwise*. For each row of O we run the exponent (e) function and normalize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss\n",
    "\n",
    "### Probalistic output\n",
    "The softmax function gives a vector y-hat which can be interpreted as a conditional probability. For example assuming that y-hat(1) represents a cat in a classification and x is any input, y-hat(1) = P(y = cat |x) or the probability that the actual output (y) is a cat given the input. \n",
    "\n",
    "We can compare these estimates to reality by checking how probable the actual classes are according to our model given the features: P(Y|X) = PI[n/i=1] P(y^^i|x^^i) assuming that each label is drawn independently from it's respective distribution.\n",
    "\n",
    "It's also important to remember that Log(AB) = Log(A) + Log(B) so we are able to modify the problem such that -log(P(Y|X)) = SIGMA[n/i=1]( -log(P(y^^i|x^^i )) ) = SIGMA[n/i=1]( l(P(y^^i,y-hat^^i )) ) our loss function l(y,y-hat) = -SIGMA[q/j=1] (y(j)log(y-hat(j)))\n",
    "\n",
    "### Softmax and cross entropy loss\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "l(y,\\hat{y}) = -\\sum_{j=1}^q y_j log(\\frac{exp(o_j)}{\\sum_{k=1}^q exp(o_k)})\n",
    "\\end{align}\n",
    "$\n",
    "Which can be further split into \n",
    "$\n",
    "\\begin{align}\n",
    " = \\sigma_{j=1}^q y_j log(\\sigma_{k=1}^q exp(o_k)) - \\sigma_{j=1}^q y_j o_j\n",
    " = log(\\sigma_{k=1}^q exp(o_k)) - \\sigma_{j=1}^q y_j o_j\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
