{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing gym: part of the open AI project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1',render_mode=\"single_rgb_array\").unwrapped # Loads the environment for open ai gym\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory\n",
    "\n",
    "- Replay memory stores the transitions that the agent observes allowing us to reuse this data later. By Sampling it randomly the transitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure.\n",
    "- We're using replay memory. The Replay memory holds a transition between two states, which is what we're using to reward our ai; our reinforcement learning. In essence we're deciding what to do next based on any given scenario, like moving for checkmate if you're able to on a chess board.\n",
    "- For example, if we wanted to know what to do in any given scenario, but we didn't want to just practice the same scenario over an over again, we'd use something like replay memory and select a random event that already happened and try to figure out what we'd do better or keep the same.\n",
    "- Kinda like chess puzzles on chess.com, this is a way of improving gradually. Or like repeating a tasks and reviewing the result over and over again.\n",
    "- Granted it's important that it is random as you can easily get stuck doing the same thing and not really improving at all. This is the best way to do things if the result is certain, who knows if the result is uncertain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward')) # Is used like a class in this example.\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN (Deep Q-Network) algorithm.\n",
    "\n",
    "- The main Idea behind Q-learning is that if we had a function (Q∗ : State×Action -> R), that could tell us what our return would be, if we were to take an action in a given state, then we could easily construct a policy that maximizes our rewards.\n",
    "- In other words, if we are able to create a function using our current state and action as parameters and our reward as a real number result, we would be able to quantify and thus maximize our rewards.\n",
    "    - in simpler words, If we can put a number on an action, we can try and shoot for a higher number.\n",
    "    - The only issues is that we can only guess that number, something NN's are effective at.\n",
    "- One example of a policy that we can use to maximize our rewards is `π ∗(s)=argmax(sub(a)) Q∗(s,a)`\n",
    "- This can apply to more than just neural networks, for example if we wanted to guess which food we'd like at a vending machine, we could quantify our answers and use the result as a way of guaging if our answers are truly the best.\n",
    "- I'm not aware of any alternatives, so I cannot critique this.\n",
    "\n",
    "\n",
    "### Bellman equation\n",
    "\n",
    "- The Bellman equation asserts that a long term rewards is equal to the rewards from the current action combined with the expected rewards from future actions.\n",
    "- In other words, We can decide rewards based on the current state and all future actions based on that current state.\n",
    "- For example, if we are trying to navigate a robot through a room, we can use the Bellman equation to decide a way to navigate through that room, based on the values we assign to each square ![For example](https://media.geeksforgeeks.org/wp-content/uploads/20210914210519/env21.png) (Credit to Geeksforgeeks)\n",
    "- The is very useful when trying to decide (on a base level) which state is the most optimal state in any given context.\n",
    "\n",
    "\n",
    "### Our NN\n",
    "\n",
    "- Our Neural network will be a convolutional neural network that takes the current and previous screen patches and outputs Q(s,left) and Q(s,right) where s is the input of the network.\n",
    "- In other words, our NN tries to predict the expected return of the outcome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self,h,w,outputs):\n",
    "        super(DQN,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    ">\n",
    "> convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (72233638.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [23]\u001b[1;36m\u001b[0m\n\u001b[1;33m    _, screen_height, screen_width = screen.\u001b[0m\n\u001b[1;37m                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render().transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'transpose'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Scott Hamilton\\Documents\\Repos\\Machine-Learning-Mini-Projects\\ReinforcementLearning\\DQNCartPole.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000009?line=5'>6</a>\u001b[0m TARGET_UPDATE \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000009?line=7'>8</a>\u001b[0m \u001b[39m# Get screen size so that we can initialize layers correctly based on shape\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000009?line=8'>9</a>\u001b[0m \u001b[39m# returned from AI gym. Typical dimensions at this point are close to 3x40x90\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000009?line=9'>10</a>\u001b[0m \u001b[39m# which is the result of a clamped and down-scaled render buffer in get_screen()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000009?line=10'>11</a>\u001b[0m init_screen \u001b[39m=\u001b[39m get_screen()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000009?line=11'>12</a>\u001b[0m _, _, screen_height, screen_width \u001b[39m=\u001b[39m init_screen\u001b[39m.\u001b[39mshape\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000009?line=13'>14</a>\u001b[0m \u001b[39m# Get number of actions from gym action space\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Scott Hamilton\\Documents\\Repos\\Machine-Learning-Mini-Projects\\ReinforcementLearning\\DQNCartPole.ipynb Cell 10\u001b[0m in \u001b[0;36mget_screen\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000009?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_screen\u001b[39m():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000009?line=11'>12</a>\u001b[0m     \u001b[39m# Returned screen requested by gym is 400x600x3, but is sometimes larger\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000009?line=12'>13</a>\u001b[0m     \u001b[39m# such as 800x1200x3. Transpose it into torch order (CHW).\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000009?line=13'>14</a>\u001b[0m     screen \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mrender()\u001b[39m.\u001b[39;49mtranspose((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000009?line=14'>15</a>\u001b[0m     \u001b[39m# Cart is in the lower half, so strip off the top and bottom of the screen\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000009?line=15'>16</a>\u001b[0m     _, screen_height, screen_width \u001b[39m=\u001b[39m screen\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'transpose'"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'transpose'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Scott Hamilton\\Documents\\Repos\\Machine-Learning-Mini-Projects\\ReinforcementLearning\\DQNCartPole.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000011?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m i_episode \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_episodes):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000011?line=2'>3</a>\u001b[0m     \u001b[39m# Initialize the environment and state\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000011?line=3'>4</a>\u001b[0m     env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000011?line=4'>5</a>\u001b[0m     last_screen \u001b[39m=\u001b[39m get_screen()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000011?line=5'>6</a>\u001b[0m     current_screen \u001b[39m=\u001b[39m get_screen()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000011?line=6'>7</a>\u001b[0m     state \u001b[39m=\u001b[39m current_screen \u001b[39m-\u001b[39m last_screen\n",
      "\u001b[1;32mc:\\Users\\Scott Hamilton\\Documents\\Repos\\Machine-Learning-Mini-Projects\\ReinforcementLearning\\DQNCartPole.ipynb Cell 12\u001b[0m in \u001b[0;36mget_screen\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000011?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_screen\u001b[39m():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000011?line=11'>12</a>\u001b[0m     \u001b[39m# Returned screen requested by gym is 400x600x3, but is sometimes larger\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000011?line=12'>13</a>\u001b[0m     \u001b[39m# such as 800x1200x3. Transpose it into torch order (CHW).\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000011?line=13'>14</a>\u001b[0m     screen \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mrender()\u001b[39m.\u001b[39;49mtranspose((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000011?line=14'>15</a>\u001b[0m     \u001b[39m# Cart is in the lower half, so strip off the top and bottom of the screen\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Scott%20Hamilton/Documents/Repos/Machine-Learning-Mini-Projects/ReinforcementLearning/DQNCartPole.ipynb#ch0000011?line=15'>16</a>\u001b[0m     _, screen_height, screen_width \u001b[39m=\u001b[39m screen\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'transpose'"
     ]
    }
   ],
   "source": [
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "80ca3b8c96389472cfc8a3d17b6b6c30e7062ea14d74503373ed131c0d8a118b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
