{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'e', 1: 'm', 2: 'o', 3: 'w', 4: ' ', 5: 'g', 6: 'd', 7: 'h', 8: 'c', 9: 'u', 10: 'a', 11: 'i', 12: 'r', 13: 'y', 14: 'f', 15: 'v', 16: 'n'} , {'e': 0, 'm': 1, 'o': 2, 'w': 3, ' ': 4, 'g': 5, 'd': 6, 'h': 7, 'c': 8, 'u': 9, 'a': 10, 'i': 11, 'r': 12, 'y': 13, 'f': 14, 'v': 15, 'n': 16}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "#credit goes to https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/\n",
    "text = ['hey how are you','good i am fine','have a nice day']\n",
    "\n",
    "chars = set(''.join(text))\n",
    "\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "char2int = {char: ind for ind, char in int2char.items()}\n",
    "print(int2char,\",\",char2int)\n",
    "\n",
    "maxlen = len(max(text,key=len))\n",
    "\n",
    "# Padding\n",
    "\n",
    "# A simple loop that loops through the list of sentences and adds a ' ' whitespace until the length of\n",
    "# the sentence matches the length of the longest sentence\n",
    "for i in range(len(text)):\n",
    "  while len(text[i])<maxlen:\n",
    "      text[i] += ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> import numpy as np\n",
    "> import torch\n",
    "\n",
    "Basic imports it's very useful.\n",
    "\n",
    "> chars = set(''.join(text))\n",
    "\n",
    "the \"set function\" turns a string into a set (a list with non repeatable elements) it uses the string/character called upon as a separator for each element.\n",
    "In this case we're turning all of the characters into a set so we use integer identification for each letter and vice versa. (char to int)\n",
    "It's similar to basic arrays or any sort of unique identification number. it's very useful to achieve what we're trying to do, which is guess the final few characters given only part of a sentence. One alternative I can think of is to iterate through the entire array and add the seperator, but that would take too long, and would be far less readable.\n",
    "\n",
    "> int2char = dict(enumerate(chars))\n",
    "\n",
    "2 functions worth eplaining here\n",
    "\n",
    "### dict()\n",
    "\n",
    "Dict **is** a function that turns an iterable into a dictionary. In other words, **it's** a hash set that turns an iterable with a key value pair into a dictionary. **For example**, If we had an iterable that already has a kv pair called \"example\" all we would need to do is call `dict(example)` and we would be able to use the dictonary as a hash map. Another way we could use it, if we didn't have an iterable that was a key value pair, is to used something simlar to named paramaters `dict(x=1,y=2,z=3)` where *x, y and z* are all variables. **Each of them** have their best use case, but of course you should use dict as you would use a hash map. **It's very useful** to use this function like named parameters, although I prefer another method which would use a constructor like in C# (although I can easily get used to this)\n",
    "\n",
    "### enumerate()\n",
    "\n",
    "Enumerate **creates** an indexed list. There are iterables in python that, by default, are not enumerated, this function **attempts to** add indexes to each entry into an array. **For example** lets say that we have a set (which by default isn't enumerated) of words that we need to access individually or that need to be modified, we can call enumerate by using `enumerate(x)` to turn set x into an enumerated list. Now we can call `x[y]` in order to access element y from set x. **An Alternative** would be to, of course, iterate over the set, although I'll admit I am not sure if that would work. **I think** it's a workable solution, I can't really think of an alternative as I'm not familar with iteration with python.\n",
    "\n",
    "> char2int = {char: ind for ind, char in int2char.items()}\n",
    "\n",
    "## What?\n",
    "### python for loops\n",
    "\n",
    "python for loops can be written like: `for x in y:` where \"x\" is the value inside of iterable y. **it's similar** to a C# foreach loop (`foreach(Type x in y)`) where x is the value and y is the iterable. **It can be used** to iterate through any list. The syntax **is no where near** as straight forward as other syntaxes in this language, although I guess other syntaxes may be more verbose. There is something similar for iterables with a key value pair(`for x,y in z`) where z is an iterable with a key value pair, x is the key and y is the value. For example, given iterable `z = {\"one\":1,\"two\":2,\"three\":3}` the first iteration of `for x,y in z: (line break) print(x,\":\",y)` would output one:1. **I think** that this code is very confusing and requires knowing how it works in order to be readable, it is brief, but i may just stick to other more clear methods of writing this, or just use comments.\n",
    "\n",
    "### {char: ind for ind, char in int2char.items()}\n",
    "\n",
    "This **mess** is a combination of an array literal `{1,2,3}` and the for loop mentioned before. **In this case** we are creating a dictionary that is the inverse of out int2char array. Although this code **is** shorthand for mapping to arrays of equal length together. It's **very useful** albeit very confusing. \n",
    "\n",
    "\n",
    "# So what does this do\n",
    "\n",
    "On a broader scale, this part of the code consists of definitions that we will use in order to simplify the entire process and void writing redundant code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: hey how are yo\n",
      "Target Sequence: ey how are you\n",
      "Input Sequence: good i am fine\n",
      "Target Sequence: ood i am fine \n",
      "Input Sequence: have a nice da\n",
      "Target Sequence: ave a nice day\n"
     ]
    }
   ],
   "source": [
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    input_seq.append(text[i][:-1])\n",
    "\n",
    "    target_seq.append(text[i][1:])\n",
    "    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))\n",
    "for i in range(len(text)):\n",
    "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
    "    target_seq[i] = [char2int[character] for character in target_seq[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> text[i][:-1]\n",
    "\n",
    "### py array slicing\n",
    "\n",
    "**There are** several parts to array indexing in python the, start, end, and step. The syntax **is** `[start:end:step]`. **For example** if we wanted to take the string `x=\"ThisThis is a long sentence in someparts of the world.world.\"` and slice it to get `\"This is a long sentence in someparts of the world.\"` we could slice x into `x[5:-6]`. There's more, if we only want every other letter we can use `x[::2]` or to write it in reverse `x[::-1]`. It's **similar** to substrings in other languages and equally as confusing. **Thankfully** the syntax is shorter and you don't have to deal with the annoyance of typing `substr(5,x.length -1)` every time.\n",
    "\n",
    "# So what does this do?\n",
    "\n",
    "It's more utilities that help use achieve our goal of finishing the sentence, It gives us a target sentence and possible inputs. Similar to how you would teach a neural network basic classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "batch_size = len(text)\n",
    "\n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "    \n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = one_hot_encode(input_seq,dict_size,seq_len,batch_size)\n",
    "\n",
    "input_seq = torch.from_numpy(input_seq)\n",
    "target_seq = torch.Tensor(target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this do\n",
    "\n",
    "This sets up a recurrent neural network with one layer. Recurrent neural networks require hidden layers as well as the normal input layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "model.to(device)\n",
    "\n",
    "# Define hyperparameters\n",
    "n_epochs = 100\n",
    "lr=0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> torch.nn.CrossEntropyLoss()\n",
    "\n",
    "Cross entropy loss **is** a loss function that measures the difference of probabilities to the desired output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100............. Loss: 2.4254\n",
      "Epoch: 20/100............. Loss: 2.1465\n",
      "Epoch: 30/100............. Loss: 1.7464\n",
      "Epoch: 40/100............. Loss: 1.3114\n",
      "Epoch: 50/100............. Loss: 0.9258\n",
      "Epoch: 60/100............. Loss: 0.6227\n",
      "Epoch: 70/100............. Loss: 0.4086\n",
      "Epoch: 80/100............. Loss: 0.2724\n",
      "Epoch: 90/100............. Loss: 0.1946\n",
      "Epoch: 100/100............. Loss: 0.1492\n"
     ]
    }
   ],
   "source": [
    "# Training Run\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    input_seq.to(device)\n",
    "    output, hidden = model(input_seq)\n",
    "    loss = criterion(output, target_seq.view(-1).long())\n",
    "    loss.backward() # Does backpropagation and calculates gradients\n",
    "    optimizer.step() # Updates the weights accordingly\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in the model and character as arguments and returns the next character prediction and hidden state\n",
    "def predict(model, character):\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    character = np.array([[char2int[c] for c in character]])\n",
    "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
    "    character = torch.from_numpy(character)\n",
    "    character.to(device)\n",
    "    \n",
    "    out, hidden = model(character)\n",
    "\n",
    "    prob = torch.nn.functional.softmax(out[-1], dim=0).data\n",
    "    # Taking the class with the highest probability score from the output\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "\n",
    "    return int2char[char_ind], hidden\n",
    "# This function takes the desired output length and input characters as arguments, returning the produced sentence\n",
    "def sample(model, out_len, start='hey'):\n",
    "    model.eval() # eval mode\n",
    "    start = start.lower()\n",
    "    # First off, run through the starting characters\n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(model, chars)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good i am fine '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(model, 15, 'good')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "80ca3b8c96389472cfc8a3d17b6b6c30e7062ea14d74503373ed131c0d8a118b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
